{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58be1df-50c7-4068-8fd9-7670f9ea72b0",
   "metadata": {},
   "source": [
    "![](../docs/ae_logo.png \"Adapt & Enable\")\n",
    "# AE workshop 2023 - Data science\n",
    "\n",
    "## Part 2 - Time Series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9e858-8579-46e8-8336-51510732cd36",
   "metadata": {},
   "source": [
    "We've explored the data and got a feel for it. Let's move on, and see if we can build a model that predicts the temperature for future points in time! \n",
    "\n",
    "First things first, let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e4905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly import offline\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import *\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    ")\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "import warnings\n",
    "from tqdm import TqdmExperimentalWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=TqdmExperimentalWarning)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "ae_orange = \"#FD9129\"\n",
    "ae_orange2 = \"#FFD580\"\n",
    "ae_gold = \"#FFD700\"\n",
    "\n",
    "!mkdir ../mlruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc53e2-c306-4e6c-9bdf-a6575054a6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(Path(\"../mlruns\").absolute().as_uri())\n",
    "print(mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab1139-cfa1-4366-9c0d-9ecc71dfb11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"ae_forecasting_workshop\"\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        name=experiment_name,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "print(f\"Experiment id: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29a379",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2eae3f",
   "metadata": {},
   "source": [
    "Let's load the data we saved from the previous step. If the next cell fails, *make sure you ran through the entire [EDA notebook](./1_EDA.ipynb)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174f665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cleaned.csv\")\n",
    "df.index = df.date\n",
    "df = df.drop(columns=[\"date\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434cff69-20d6-4a58-acbc-3027ea646f3e",
   "metadata": {},
   "source": [
    "Alright, looks good! Let's get started. \n",
    "\n",
    "### Train-val-test split\n",
    "When we train models, we need a way to know if they're worth a damn. \n",
    "\n",
    "<span style=\"color:#FD9129\">**How would you approach this?**</span>\n",
    "\n",
    "<span style=\"color:#FD9129\">**Time series are a special case. Any idea why?**</span>\n",
    "\n",
    "<span style=\"color:#FD9129\">**How can we mitigate this issue?**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7b62d-9e0b-4bba-a492-9fa6ab0404c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to generate splits\n",
    "def train_val_test_split(data: pd.DataFrame, train=0.7, val=0.15, test=0.15):\n",
    "    # Make sure our splits add up\n",
    "    total = train + test + val\n",
    "    if not total == 1:\n",
    "        print(\n",
    "            f\"Train, validation and test portions don't add up to 1! Currently the total is{total}. Rebalancing.\"\n",
    "        )\n",
    "        train /= total\n",
    "        val /= total\n",
    "        test /= total\n",
    "\n",
    "    # Calculate row counts\n",
    "    n_rows = len(data)\n",
    "    train_rows = int(train * n_rows)\n",
    "    val_rows = int(val * n_rows)\n",
    "    test_rows = n_rows - (train_rows + val_rows)\n",
    "\n",
    "    # Slice up the data\n",
    "    train_data = data.iloc[:train_rows]\n",
    "    val_data = data.iloc[train_rows : train_rows + val_rows]\n",
    "    test_data = data.iloc[train_rows + val_rows :]\n",
    "\n",
    "    print(\n",
    "        f\"Training split:   {train_rows} data points\\tfrom {train_data.index.min()} till {train_data.index.max()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation split: {val_rows} data points\\tfrom {val_data.index.min()} till {val_data.index.max()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Test split:       {test_rows} data points\\tfrom {test_data.index.min()} till {test_data.index.max()}\"\n",
    "    )\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320db99-46b8-4cd5-8b7d-4c6734f6ecde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, val, test = train_val_test_split(df, train=0.8, val=0.2, test=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9846cc1",
   "metadata": {},
   "source": [
    "We need metrics to quantify the performance of our models. \n",
    "\n",
    "<span style=\"color:#FD9129\">**In our case, what could be good metrics?**</span>\n",
    "\n",
    "Here's some inspiration: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34305487",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to give us some basic time series forecasting performance metrics\n",
    "def forecast_metrics(y_true, y_pred):\n",
    "    \n",
    "    # Collect metrics in a dictionary\n",
    "    metrics = {}\n",
    "    \n",
    "    # Add your own metrics!\n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "    #metrics[\"my_metric\"] = some_metric_calculation_function(y_true, y_pred)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818e2df-7ee1-49fc-9d24-3601a247b7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's see if they work\n",
    "a = [1,2,3,4,5,7,8,9,11]\n",
    "b = [1,3,3,4,5,6,7,9,11]\n",
    "print(forecast_metrics(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edbc75",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to visualize performance\n",
    "def visualize_metrics(results_df: pd.DataFrame, baseline=True):\n",
    "    # Don't mess with our original data\n",
    "    viz_df = results_df.copy()\n",
    "\n",
    "    # Get rid of baseline if not requested\n",
    "    if not baseline:\n",
    "        del viz_df[\"HistoricAverage\"]\n",
    "\n",
    "    # Create subplot grid with one row per metric\n",
    "    fig = make_subplots(rows=1, cols=viz_df.shape[0], horizontal_spacing=0.1)\n",
    "\n",
    "    # Loop through each row and add a bar chart to the corresponding subplot\n",
    "    for idx, (name, row) in enumerate(viz_df.iterrows()):\n",
    "        fig.add_trace(go.Bar(x=row.index, y=row.values, name=name), row=1, col=idx + 1)\n",
    "        # Give the child a name\n",
    "        fig.update_yaxes(title_text=name, row=1, col=idx + 1)\n",
    "\n",
    "    # Update layout and show the figure\n",
    "    fig.update_layout(\n",
    "        height=400, width=1200, title=\"Model comparison\", showlegend=False\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca41b5",
   "metadata": {},
   "source": [
    "### 1. Statistical modelling: ARIMA (and friends)\n",
    "\n",
    "We'll first try and fit an **A**uto**r**egressive **I**ntegrated **M**oving **A**verage model. That's a mouthful, we know. This statistical model is quite common in the industry (e.g., in stock price forecasting).\n",
    "\n",
    "* It's *autoregressive* because it uses past values of a variable to predict its future values (i.e., it regresses onto itself).\n",
    "* It uses a *moving average* of the past errors to predict future errors.\n",
    "* Finally, it's *integrated* in the sense that it uses differencing to make the data stationary -- that is, it removes trends and seasonal patterns to try and obtain a stationary signal.\n",
    "\n",
    "Let's fit one! Here, we will use a class called `AutoARIMA`, which automatically finds the best ARIMA model based on an information criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf6f06",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Many of these packages are picky about the column names, so here's a helper function to prep the data\n",
    "def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={\"date\": \"ds\", \"temperature\": \"y\"})\n",
    "    df[\"unique_id\"] = 1\n",
    "    df = df[[\"unique_id\", \"ds\", \"y\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361bf7d7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sf = prep_df(train)\n",
    "val_sf = prep_df(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d5b18-4a4f-47ad-846e-7a3215e0f59e",
   "metadata": {},
   "source": [
    "<span style=\"color:#FD9129\">*Before we go into model fitting, a word on experiment tracking!*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392385a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "season_length = 12  # Monthly data\n",
    "horizon = len(val)  # Predict the length of the validation df\n",
    "\n",
    "# Include the models you imported\n",
    "models = [\n",
    "    # This is a baseline model\n",
    "    HistoricAverage(),\n",
    "    # This is a better model\n",
    "    AutoARIMA(season_length=season_length),\n",
    "    # YOU CAN ADD MODELS HERE\n",
    "]\n",
    "\n",
    "# We'll store metrics here\n",
    "metrics = dict()\n",
    "\n",
    "# We'll keep track of our forecasts for visualization\n",
    "forecasts = dict()\n",
    "\n",
    "# Let's loop through the models\n",
    "for m in tqdm(models, desc=\"Fitting models\"):\n",
    "    \n",
    "    # Start an MLflow run for each model\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name=str(m)):\n",
    "        \n",
    "        # Instantiate the StatsForecast class as sf\n",
    "        model = StatsForecast(df=train_sf, models=[m], freq=\"MS\", n_jobs=-1)\n",
    "\n",
    "        # Forecast for the defined horizon\n",
    "        forecasts[str(m)] = model.forecast(horizon)\n",
    "\n",
    "        # Let's calculate and store metrics for every one of them\n",
    "        metrics[str(m)] = forecast_metrics(val.temperature, forecasts[str(m)][str(m)])\n",
    "\n",
    "        # Log the metrics\n",
    "        mlflow.log_metrics(metrics[str(m)])\n",
    "\n",
    "        # Log the model TODO: this is probably wrong, use pyfunc instead\n",
    "        mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678eb3db",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386351ef",
   "metadata": {},
   "source": [
    "As you can see, the forecast dict now has model-forecast mappings, corresponding to the validation dates.\n",
    "\n",
    "If you add another type of model to the list, they'll show up here as extra keys. \n",
    "\n",
    "<span style=\"color:#FD9129\">**Have a look [here](https://github.com/Nixtla/statsforecast) if you're curious, or just want to play around.**</span>\n",
    "\n",
    "Before we calculate performance metrics, let's visualize our predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920cf8e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_plots = []\n",
    "prediction_plots = []\n",
    "\n",
    "# Plot our training data\n",
    "base_plots.append(\n",
    "    go.Scatter(\n",
    "        x=train.index,\n",
    "        y=train.temperature,\n",
    "        mode=\"lines\",\n",
    "        name=\"training\",\n",
    "        line=dict(color=ae_gold),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot our observed validation values\n",
    "base_plots.append(\n",
    "    go.Scatter(\n",
    "        x=val.index,\n",
    "        y=val.temperature,\n",
    "        mode=\"lines\",\n",
    "        name=\"observed\",\n",
    "        line=dict(color=\"grey\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot our ARIMA prediction\n",
    "for model, forecast in forecasts.items():\n",
    "    prediction_plots.append(\n",
    "        go.Scatter(\n",
    "            x=forecast.ds, y=forecast[model], mode=\"lines\", name=f\"predicted_{model}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Display everything together\n",
    "fig = go.Figure(data=base_plots + prediction_plots)\n",
    "fig.update_layout(title=\"Forecasting in action ðŸš€\")\n",
    "\n",
    "fig.show(renderer=\"iframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7331d7e",
   "metadata": {},
   "source": [
    "Not bad! Let's calculate some performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb9123",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(metrics)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf0253-31e5-4a2d-8e6b-916007ee3c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_metrics(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee9e1f-0be4-4a11-9d35-7030dc4bb591",
   "metadata": {},
   "source": [
    "Clearly, the ARIMA approach blows the baseline model out of the water. Good! Now let's try another model: Facebook's [Prophet](https://facebook.github.io/prophet/)!\n",
    "\n",
    "### 2. Prophet\n",
    "\n",
    "> Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n",
    "\n",
    "Prophet is a special case of the Generalized Additive Model. Whereas ARIMA tries to build a formula for future values as a function of past values, Prophet tries to detect â€œchange pointsâ€; you can think of Prophet as curve-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3d48d-3750-4a00-9cd6-27a0442dcb4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ebb58-7ea8-4f96-9577-ebd9208cde77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll use the same train and val dataframes as before\n",
    "model = Prophet()\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"Prophet\"):\n",
    "    # Fit the model\n",
    "    model.fit(train_sf)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.prophet.log_model(model, artifact_path=\"prophet\")\n",
    "\n",
    "    # Forecast for the defined horizon\n",
    "    future = model.make_future_dataframe(periods=horizon, freq=\"MS\")\n",
    "    forecast = model.predict(future)[-len(val) :]\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics[\"Prophet\"] = forecast_metrics(val.temperature, forecast.yhat)\n",
    "\n",
    "    # Log the metrics\n",
    "    mlflow.log_metrics(metrics[\"Prophet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5bc69d-ce6c-4d1b-9296-c2d15afaf8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot prophet's prediction\n",
    "prophet_plot = [\n",
    "    go.Scatter(x=forecast.ds, y=forecast.yhat, mode=\"lines\", name=\"predicted_prophet\")\n",
    "]\n",
    "\n",
    "# Display\n",
    "fig = go.Figure(data=base_plots + prediction_plots + prophet_plot)\n",
    "fig.update_layout(title=\"Forecasting in action (part 2) ðŸš€\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adef50d-aca1-4223-9848-b36cf26a1310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(metrics)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33457bb-a8cc-44db-a12a-c391d50b5234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_metrics(results_df, baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc0198-ab3e-46a9-949a-ccd3f9c361da",
   "metadata": {
    "tags": []
   },
   "source": [
    "Interestingly, it appears that Prophet has a small edge in performance. Inspect the plot: it looks like, as time progresses, the ARIMA model is content to just keep oscillating around a certain value, and fails to capture the upwards trend well. Prophet does better in predicting the later time points. Get rid of the baseline model (HistoricAverage) to get a better look.\n",
    "\n",
    "For the hell of it, let's go bonkers with some out-of-the-box deep learning: [NeuralProphet](https://neuralprophet.com). Overkill, yay!\n",
    "\n",
    "### 3. NeuralProphet\n",
    "\n",
    "> NeuralProphet is an easy to learn framework for interpretable time series forecasting. NeuralProphet is built on PyTorch and combines Neural Network and traditional time-series algorithms, inspired by Facebook Prophet and AR-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415aba8-15d6-42ab-a7b9-d0cf75333102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from neuralprophet import NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba0c46-7ba6-4503-b852-6d716e25fcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapt our training data to fit the expected format\n",
    "train_neural = train_sf.drop(columns=[\"unique_id\"])\n",
    "\n",
    "# Initialize model\n",
    "overkill_model = NeuralProphet()\n",
    "\n",
    "# Start a new run\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"Neural Prophet\"):\n",
    "    # Fit the model\n",
    "    overkill_model.fit(train_neural)\n",
    "\n",
    "    # Forecast for the defined horizon\n",
    "    future = overkill_model.make_future_dataframe(\n",
    "        train_neural, periods=len(val), n_historic_predictions=len(train_neural)\n",
    "    )\n",
    "    forecast = overkill_model.predict(future)[-len(val) :]\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics[\"Neural Prophet\"] = forecast_metrics(val.temperature, forecast.yhat1)\n",
    "\n",
    "    # Log the metrics\n",
    "    mlflow.log_metrics(metrics[\"Neural Prophet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323f185-23e3-4f1d-bed3-c6c8e897c56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot prophet's prediction\n",
    "neural_plot = [\n",
    "    go.Scatter(\n",
    "        x=forecast.ds, y=forecast.yhat1, mode=\"lines\", name=\"predicted_neural_prophet\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Display\n",
    "fig = go.Figure(data=base_plots + prediction_plots + neural_plot + prophet_plot)\n",
    "fig.update_layout(title=\"Forecasting in action (part 2) ðŸš€\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc6730-172d-46e6-8cb8-1e16d01528c5",
   "metadata": {},
   "source": [
    "Looks pretty similar, right? Hard to tell if we gained anything. What does the data tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7610d-f01c-471d-8428-5d3f37479a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics[\"Neural Prophet\"] = forecast_metrics(val.temperature, forecast.yhat1)\n",
    "results_df = pd.DataFrame(metrics)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e820f-0144-4d40-b258-94c4c0323852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_metrics(results_df, baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d25b2d-de0a-47ee-b0eb-3c0d5715bc41",
   "metadata": {},
   "source": [
    "You'll have to zoom in again, but it looks like we reduced our error yet a little more. Worth it? Depends on the time and compute we have at hand. The AutoARIMA model was actually quite expensive on both fronts. Prophet, in comparison, did fairly well! NeuralProphet did even better, but took a while longer to fit. Use the right tool for the right job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b481a6-41ae-4c2a-9662-6307bcd3ae0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74acd39-9de7-4590-a35c-79cc0b90bd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324390a-2e48-43ec-aa35-92d9f5a5cac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env_ws",
   "language": "python",
   "name": "temp_env_ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
